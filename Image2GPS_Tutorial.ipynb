{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCI1gQ72850T"
      },
      "source": [
        "# Image2GPS: Predicting Location from Street-Level Images\n",
        "\n",
        "**A Beginner-Friendly Tutorial on Visual Geolocation with Deep Learning**\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "In this tutorial, we'll explore how deep learning models can predict *where* a photo was taken just by looking at its visual content. This task is called **visual geolocation** or **Image2GPS**.\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "1. **The Problem**: Why is predicting GPS from images challenging?\n",
        "2. **The Model**: How CNNs learn location-relevant features\n",
        "3. **Preprocessing**: Why image transforms matter\n",
        "4. **Evaluation**: How we measure geolocation accuracy\n",
        "5. **Insights**: What visual cues help the model localize?\n",
        "\n",
        "**Prerequisites**: Basic Python knowledge. Deep learning experience helpful but not required.\n",
        "\n",
        "**Authors**: Cecilia Chen, Ranty Wang, Xun Wang (CIS 5190, Fall 2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E21XBDX850U"
      },
      "source": [
        "---\n",
        "## Part 1: Understanding the Problem\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Imagine you're shown a street photo and asked: *\"Where was this taken?\"*\n",
        "\n",
        "Humans solve this by recognizing landmarks, reading signs, noticing architectural styles, or identifying vegetation patterns. Our goal is to teach a neural network to do the same!\n",
        "\n",
        "### Why It's Hard\n",
        "\n",
        "- **Fine-grained differences**: Two locations 50 meters apart may look very similar\n",
        "- **Appearance variation**: The same spot looks different at dawn vs. dusk, summer vs. winter\n",
        "- **Ambiguous scenes**: Generic sidewalks or grass could be anywhere\n",
        "\n",
        "### Our Approach\n",
        "\n",
        "We frame this as a **regression problem**:\n",
        "- **Input**: A street-level image (224x224 pixels)\n",
        "- **Output**: GPS coordinates (latitude, longitude)\n",
        "\n",
        "Let's visualize what our target region looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4tenaae850V"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q torch torchvision matplotlib numpy pandas folium geopy pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZuH9ZFI850V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiQwHL3o850W"
      },
      "outputs": [],
      "source": [
        "# Visualize our target region: Penn Campus (33rd & Walnut to 34th & Spruce)\n",
        "import folium\n",
        "\n",
        "# Define the bounding box\n",
        "bounds = {\n",
        "    'north': 39.9535,   # 34th & Spruce (north edge)\n",
        "    'south': 39.9500,   # 33rd & Walnut (south edge)\n",
        "    'east': -75.1895,   # East boundary\n",
        "    'west': -75.1945    # West boundary\n",
        "}\n",
        "\n",
        "# Create map centered on Penn\n",
        "center_lat = (bounds['north'] + bounds['south']) / 2\n",
        "center_lon = (bounds['east'] + bounds['west']) / 2\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=17)\n",
        "\n",
        "# Add bounding rectangle\n",
        "folium.Rectangle(\n",
        "    bounds=[[bounds['south'], bounds['west']], [bounds['north'], bounds['east']]],\n",
        "    color='blue',\n",
        "    fill=True,\n",
        "    fill_opacity=0.2,\n",
        "    popup='Target Region: ~400m x 350m'\n",
        ").add_to(m)\n",
        "\n",
        "# Add center marker\n",
        "folium.Marker(\n",
        "    [center_lat, center_lon],\n",
        "    popup='Center of target region',\n",
        "    icon=folium.Icon(color='red', icon='info-sign')\n",
        ").add_to(m)\n",
        "\n",
        "print(\"[MAP] Our target region on Penn's campus:\")\n",
        "print(f\"   Size: approximately 400m x 350m\")\n",
        "print(f\"   Center: ({center_lat:.4f} N, {center_lon:.4f} W)\")\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTd7koLW850W"
      },
      "source": [
        "---\n",
        "## Part 2: The Model Architecture\n",
        "\n",
        "### Why ResNet?\n",
        "\n",
        "We use **ResNet-50**, a convolutional neural network (CNN) that excels at image recognition. Here's the intuition:\n",
        "\n",
        "1. **Early layers** detect simple patterns: edges, corners, textures\n",
        "2. **Middle layers** combine these into shapes: windows, rooflines, tree canopies\n",
        "3. **Deep layers** recognize complex structures: building facades, walkway layouts\n",
        "4. **Final layer** maps these features to GPS coordinates\n",
        "\n",
        "### From Classification to Regression\n",
        "\n",
        "ResNet was designed for *classification* (e.g., \"Is this a cat or dog?\"). We modify it for *regression* by:\n",
        "- Replacing the 1000-class output with 2 values (latitude, longitude)\n",
        "- Using MSE loss instead of cross-entropy\n",
        "\n",
        "Let's build our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHCOAurN850W"
      },
      "outputs": [],
      "source": [
        "class IMG2GPS(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN that predicts GPS coordinates from images.\n",
        "\n",
        "    Architecture:\n",
        "        ResNet-50 backbone -> Custom regression head -> (lat, lon)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load ResNet-50 with ImageNet pretrained weights\n",
        "        # These weights give us a \"head start\" - the model already knows\n",
        "        # how to detect edges, textures, and shapes!\n",
        "        weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        self.backbone = models.resnet50(weights=weights)\n",
        "\n",
        "        # Get the number of features from ResNet's final layer\n",
        "        num_features = self.backbone.fc.in_features  # 2048 for ResNet-50\n",
        "\n",
        "        # Replace classification head with regression head\n",
        "        self.backbone.fc = nn.Identity()  # Remove original FC layer\n",
        "\n",
        "        # Custom regression head with regularization\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(num_features, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Prevents overfitting\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 2)  # Output: [latitude, longitude]\n",
        "        )\n",
        "\n",
        "        # GPS normalization parameters (computed from training data)\n",
        "        # We normalize coordinates to have mean=0, std=1 for stable training\n",
        "        self.lat_mean = 39.9517\n",
        "        self.lat_std = 0.00065\n",
        "        self.lon_mean = -75.1915\n",
        "        self.lon_std = 0.00063\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass - returns normalized coordinates.\"\"\"\n",
        "        features = self.backbone(x)\n",
        "        return self.regression_head(features)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Make predictions in real GPS coordinates (degrees).\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            normalized = self.forward(x)\n",
        "            # Convert back to actual GPS coordinates\n",
        "            lat = normalized[:, 0] * self.lat_std + self.lat_mean\n",
        "            lon = normalized[:, 1] * self.lon_std + self.lon_mean\n",
        "            return torch.stack([lat, lon], dim=1)\n",
        "\n",
        "# Create model instance\n",
        "model = IMG2GPS(pretrained=True)\n",
        "print(\"[OK] Model created!\")\n",
        "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWTfGec850W"
      },
      "source": [
        "### Visualizing the Architecture\n",
        "\n",
        "Let's see what's inside our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o03vzvhD850W"
      },
      "outputs": [],
      "source": [
        "# Visualize model structure\n",
        "def count_parameters(module):\n",
        "    return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "print(\"[INFO] Model Architecture Overview\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\n>> ResNet-50 Backbone\")\n",
        "print(f\"   - Initial conv + pooling: {count_parameters(model.backbone.conv1) + count_parameters(model.backbone.bn1):,} params\")\n",
        "print(f\"   - Layer 1 (64 channels):  {count_parameters(model.backbone.layer1):,} params\")\n",
        "print(f\"   - Layer 2 (128 channels): {count_parameters(model.backbone.layer2):,} params\")\n",
        "print(f\"   - Layer 3 (256 channels): {count_parameters(model.backbone.layer3):,} params\")\n",
        "print(f\"   - Layer 4 (512 channels): {count_parameters(model.backbone.layer4):,} params\")\n",
        "print(f\"\\n>> Regression Head\")\n",
        "print(f\"   - Dense layers: {count_parameters(model.regression_head):,} params\")\n",
        "print(f\"\\n   Input: Image (3x224x224) -> Output: GPS (lat, lon)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV0cH4As850X"
      },
      "source": [
        "---\n",
        "## Part 3: Image Preprocessing\n",
        "\n",
        "### Why Preprocessing Matters\n",
        "\n",
        "Raw images vary in size, lighting, and orientation. We need to standardize them:\n",
        "\n",
        "1. **Resize** to 224x224 (ResNet's expected input size)\n",
        "2. **Normalize** pixel values (ImageNet statistics)\n",
        "3. **Augment** training images for robustness\n",
        "\n",
        "### Our Key Finding: Grayscale Helps!\n",
        "\n",
        "Through experiments, we discovered that **randomly converting images to grayscale** (10% of the time during training) improves accuracy by ~1.8%!\n",
        "\n",
        "**Why?** Color can be misleading:\n",
        "- The same building looks different at sunrise vs. sunset\n",
        "- Seasonal changes alter foliage colors\n",
        "- Weather affects color perception\n",
        "\n",
        "By occasionally removing color, we force the model to focus on **geometry and structure** - features that are more stable for localization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDtLnpB4850X"
      },
      "outputs": [],
      "source": [
        "# Define our preprocessing pipelines\n",
        "\n",
        "# Training: With augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop for variety\n",
        "    transforms.RandomHorizontalFlip(p=0.5),              # Flip left-right\n",
        "    transforms.RandomRotation(degrees=15),               # Small rotations\n",
        "    transforms.ColorJitter(                              # Lighting variation\n",
        "        brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1\n",
        "    ),\n",
        "    transforms.RandomGrayscale(p=0.1),                   # << Key augmentation!\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(                                # ImageNet statistics\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Inference: No augmentation (deterministic)\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "print(\"[OK] Preprocessing pipelines defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mee3o50Q850X"
      },
      "outputs": [],
      "source": [
        "# Visualize augmentation effects\n",
        "# We'll create a sample image to demonstrate\n",
        "\n",
        "# Create a synthetic \"street scene\" for demonstration\n",
        "np.random.seed(42)\n",
        "sample_img = np.zeros((300, 400, 3), dtype=np.uint8)\n",
        "\n",
        "# Sky (blue gradient)\n",
        "for i in range(100):\n",
        "    sample_img[i, :] = [135 + i//2, 206, 235]\n",
        "\n",
        "# Building (brown/tan)\n",
        "sample_img[100:250, 50:150] = [139, 119, 101]\n",
        "sample_img[100:250, 250:350] = [160, 140, 120]\n",
        "\n",
        "# Windows\n",
        "for y in range(120, 240, 40):\n",
        "    for x in [70, 110, 270, 310]:\n",
        "        sample_img[y:y+25, x:x+20] = [200, 220, 255]\n",
        "\n",
        "# Ground (gray sidewalk)\n",
        "sample_img[250:, :] = [128, 128, 128]\n",
        "\n",
        "# Trees (green)\n",
        "sample_img[150:250, 175:225] = [34, 139, 34]\n",
        "\n",
        "sample_pil = Image.fromarray(sample_img)\n",
        "\n",
        "# Show original and augmented versions\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
        "fig.suptitle('Augmentation Effects on a Street Scene', fontsize=14)\n",
        "\n",
        "# Original\n",
        "axes[0, 0].imshow(sample_img)\n",
        "axes[0, 0].set_title('Original')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Various augmentations\n",
        "aug_names = ['Random Crop', 'Horizontal Flip', 'Color Jitter',\n",
        "             'Grayscale', 'Rotation', 'Combined Aug', 'Final Input']\n",
        "\n",
        "torch.manual_seed(42)\n",
        "augmentations = [\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=1.0),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                        transforms.ColorJitter(brightness=0.3)]),\n",
        "    train_transform,\n",
        "]\n",
        "\n",
        "for idx, (name, aug) in enumerate(zip(aug_names, augmentations)):\n",
        "    row, col = (idx + 1) // 4, (idx + 1) % 4\n",
        "\n",
        "    try:\n",
        "        augmented = aug(sample_pil)\n",
        "        if isinstance(augmented, torch.Tensor):\n",
        "            # Denormalize for visualization\n",
        "            img_show = augmented.permute(1, 2, 0).numpy()\n",
        "            img_show = img_show * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img_show = np.clip(img_show, 0, 1)\n",
        "        else:\n",
        "            img_show = np.array(augmented)\n",
        "\n",
        "        axes[row, col].imshow(img_show)\n",
        "        axes[row, col].set_title(name)\n",
        "    except:\n",
        "        axes[row, col].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
        "\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[INSIGHT] Key insight: Grayscale removes color but preserves structure!\")\n",
        "print(\"   This helps the model focus on building geometry and spatial layout.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Q_kHRy850X"
      },
      "source": [
        "---\n",
        "## Part 4: Evaluation Metrics\n",
        "\n",
        "### The Haversine Distance\n",
        "\n",
        "We can't just use Euclidean distance for GPS coordinates because the Earth is a sphere! The **Haversine formula** calculates the great-circle distance between two points:\n",
        "\n",
        "$$d = 2R \\cdot \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $R$ = Earth's radius (6,371 km)\n",
        "- $\\phi$ = latitude in radians\n",
        "- $\\lambda$ = longitude in radians\n",
        "\n",
        "### Why Not Just MSE?\n",
        "\n",
        "Mean Squared Error on raw coordinates has issues:\n",
        "1. A 0.001 degree error in latitude is not equal to 0.001 degree error in longitude (different physical distances)\n",
        "2. Doesn't give intuitive \"meters\" interpretation\n",
        "\n",
        "Haversine gives us the **actual physical distance** in meters - much more meaningful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diz3QLqT850Y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def haversine_distance(pred_lat, pred_lon, true_lat, true_lon):\n",
        "    \"\"\"\n",
        "    Calculate the great-circle distance between two GPS coordinates.\n",
        "\n",
        "    Args:\n",
        "        pred_lat, pred_lon: Predicted coordinates (degrees)\n",
        "        true_lat, true_lon: Ground truth coordinates (degrees)\n",
        "\n",
        "    Returns:\n",
        "        Distance in meters\n",
        "    \"\"\"\n",
        "    R = 6_371_000  # Earth's radius in meters\n",
        "\n",
        "    # Convert to radians\n",
        "    lat1, lon1 = math.radians(pred_lat), math.radians(pred_lon)\n",
        "    lat2, lon2 = math.radians(true_lat), math.radians(true_lon)\n",
        "\n",
        "    # Haversine formula\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "    return R * c\n",
        "\n",
        "\n",
        "def calculate_rmse(predictions, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculate RMSE of Haversine distances.\n",
        "\n",
        "    Args:\n",
        "        predictions: Array of shape (N, 2) with [lat, lon]\n",
        "        ground_truth: Array of shape (N, 2) with [lat, lon]\n",
        "\n",
        "    Returns:\n",
        "        RMSE in meters\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "    for pred, true in zip(predictions, ground_truth):\n",
        "        d = haversine_distance(pred[0], pred[1], true[0], true[1])\n",
        "        distances.append(d)\n",
        "\n",
        "    distances = np.array(distances)\n",
        "    rmse = np.sqrt(np.mean(distances**2))\n",
        "    return rmse, distances\n",
        "\n",
        "\n",
        "# Demonstrate with examples\n",
        "print(\"[DEMO] Haversine Distance Examples\\n\")\n",
        "\n",
        "# Penn campus center\n",
        "center = (39.9517, -75.1915)\n",
        "\n",
        "examples = [\n",
        "    ((39.9517, -75.1915), \"Same point\"),\n",
        "    ((39.9518, -75.1915), \"~11m north\"),\n",
        "    ((39.9517, -75.1914), \"~8m east\"),\n",
        "    ((39.9520, -75.1910), \"~50m away\"),\n",
        "    ((39.9530, -75.1900), \"~200m away\"),\n",
        "]\n",
        "\n",
        "for (lat, lon), desc in examples:\n",
        "    d = haversine_distance(lat, lon, center[0], center[1])\n",
        "    print(f\"  {desc:20s} -> {d:7.2f} meters\")\n",
        "\n",
        "print(\"\\n[NOTE] 0.0001 degrees is approximately 11 meters at this latitude!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY4C0kKy850Y"
      },
      "source": [
        "---\n",
        "## Part 5: What Does the Model Learn?\n",
        "\n",
        "Let's peek inside the CNN to understand what visual features it uses for localization.\n",
        "\n",
        "### Feature Visualization\n",
        "\n",
        "We'll extract intermediate feature maps to see what the model \"sees\" at different layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzXD2kVX850Y"
      },
      "outputs": [],
      "source": [
        "# Feature map visualization\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Extract intermediate feature maps from ResNet.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.backbone = model.backbone\n",
        "        self.features = {}\n",
        "\n",
        "        # Register hooks to capture intermediate outputs\n",
        "        self.backbone.layer1.register_forward_hook(self._get_hook('layer1'))\n",
        "        self.backbone.layer2.register_forward_hook(self._get_hook('layer2'))\n",
        "        self.backbone.layer3.register_forward_hook(self._get_hook('layer3'))\n",
        "        self.backbone.layer4.register_forward_hook(self._get_hook('layer4'))\n",
        "\n",
        "    def _get_hook(self, name):\n",
        "        def hook(module, input, output):\n",
        "            self.features[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    def forward(self, x):\n",
        "        _ = self.backbone(x)\n",
        "        return self.features\n",
        "\n",
        "# Create feature extractor\n",
        "feature_extractor = FeatureExtractor(model)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Process our sample image\n",
        "sample_tensor = inference_transform(sample_pil).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = feature_extractor(sample_tensor)\n",
        "\n",
        "# Visualize feature maps\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "fig.suptitle('What the CNN \"Sees\" at Different Layers', fontsize=14)\n",
        "\n",
        "# Original image\n",
        "axes[0, 0].imshow(sample_img)\n",
        "axes[0, 0].set_title('Input Image')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Feature maps from each layer\n",
        "layer_info = [\n",
        "    ('layer1', 'Layer 1: Edges & Textures'),\n",
        "    ('layer2', 'Layer 2: Patterns & Shapes'),\n",
        "    ('layer3', 'Layer 3: Object Parts'),\n",
        "    ('layer4', 'Layer 4: High-Level Features'),\n",
        "]\n",
        "\n",
        "for idx, (layer_name, title) in enumerate(layer_info):\n",
        "    feat = features[layer_name][0]  # Get first (only) batch item\n",
        "\n",
        "    # Average across channels for visualization\n",
        "    feat_avg = feat.mean(dim=0).cpu().numpy()\n",
        "\n",
        "    row = (idx + 1) // 4\n",
        "    col = (idx + 1) % 4\n",
        "\n",
        "    axes[row, col].imshow(feat_avg, cmap='viridis')\n",
        "    axes[row, col].set_title(f\"{title}\\n({feat.shape[0]} channels, {feat.shape[1]}x{feat.shape[2]})\")\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "# Show individual channels from layer 1\n",
        "axes[1, 1].imshow(features['layer1'][0, 0].cpu().numpy(), cmap='gray')\n",
        "axes[1, 1].set_title('Layer 1, Channel 0\\n(Edge detection)')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "axes[1, 2].imshow(features['layer1'][0, 32].cpu().numpy(), cmap='gray')\n",
        "axes[1, 2].set_title('Layer 1, Channel 32\\n(Texture detection)')\n",
        "axes[1, 2].axis('off')\n",
        "\n",
        "axes[1, 3].imshow(features['layer2'][0, :16].mean(dim=0).cpu().numpy(), cmap='plasma')\n",
        "axes[1, 3].set_title('Layer 2, Avg Channels 0-15\\n(Building structures)')\n",
        "axes[1, 3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[INTERPRETATION]\")\n",
        "print(\"   - Early layers detect edges (building outlines, windows)\")\n",
        "print(\"   - Middle layers recognize shapes (rooflines, sidewalk patterns)\")\n",
        "print(\"   - Deep layers capture high-level structure (overall scene layout)\")\n",
        "print(\"   - The model combines all these to predict location!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK6prv8u850Y"
      },
      "source": [
        "---\n",
        "## Part 6: Our Results\n",
        "\n",
        "### Performance Summary\n",
        "\n",
        "After training on 936 images from Penn's campus, our model achieves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgtHD42B850Y"
      },
      "outputs": [],
      "source": [
        "# Our experimental results\n",
        "results = {\n",
        "    'Baseline (ResNet-18, scratch)': 69.76,\n",
        "    'Baseline (ResNet-18, pretrained)': 48.52,\n",
        "    'Improved (ResNet-18, grayscale)': 33.01,\n",
        "    'Improved (ResNet-50, grayscale)': 32.53,\n",
        "    'Final (ResNet-50, 50 epochs)': 21.92,\n",
        "}\n",
        "\n",
        "# Visualize improvement\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "colors = ['#ff6b6b', '#ffa94d', '#69db7c', '#4dabf7', '#9775fa']\n",
        "bars = ax1.barh(list(results.keys()), list(results.values()), color=colors)\n",
        "ax1.set_xlabel('Test RMSE (meters) - Lower is better')\n",
        "ax1.set_title('Model Performance Comparison')\n",
        "ax1.axvline(x=50, color='green', linestyle='--', alpha=0.7, label='Excellent (<50m)')\n",
        "ax1.axvline(x=88, color='red', linestyle='--', alpha=0.7, label='Baseline requirement')\n",
        "ax1.legend()\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, results.values()):\n",
        "    ax1.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}m',\n",
        "             va='center', fontsize=10)\n",
        "\n",
        "# Training progress (milestone comparison)\n",
        "epochs = [10, 20, 30, 40, 50]\n",
        "no_gray = [35.33, 29.14, 24.12, 22.64, 22.61]\n",
        "with_gray = [36.38, 26.27, 23.67, 23.30, 21.92]\n",
        "\n",
        "ax2.plot(epochs, no_gray, 'o-', label='Without grayscale', linewidth=2, markersize=8)\n",
        "ax2.plot(epochs, with_gray, 's-', label='With grayscale', linewidth=2, markersize=8)\n",
        "ax2.fill_between(epochs, no_gray, with_gray, alpha=0.2,\n",
        "                  where=[ng > wg for ng, wg in zip(no_gray, with_gray)], color='green')\n",
        "ax2.set_xlabel('Training Epochs')\n",
        "ax2.set_ylabel('Test RMSE (meters)')\n",
        "ax2.set_title('Effect of Grayscale Augmentation Over Training')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[KEY FINDINGS]\")\n",
        "print(f\"   * Pretrained weights improved performance by 30% (69.76m -> 48.52m)\")\n",
        "print(f\"   * Grayscale augmentation provided consistent benefits\")\n",
        "print(f\"   * Final model: 21.92m RMSE - about 60% better than baseline!\")\n",
        "print(f\"   * This means on average, we can locate photos within ~22 meters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha391MyZ850Z"
      },
      "source": [
        "### Ablation Study: What Helps vs. What Hurts?\n",
        "\n",
        "We systematically tested different augmentation strategies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS_DC-yq850Z"
      },
      "outputs": [],
      "source": [
        "# Ablation results\n",
        "ablation = {\n",
        "    'Base only': (31.58, 0),\n",
        "    'Base + Grayscale': (31.02, -1.8),\n",
        "    'Base + Blur': (37.94, 20.1),\n",
        "    'Base + Erasing': (33.35, 5.6),\n",
        "    'Base + Gray + Blur': (33.09, 4.8),\n",
        "    'Full augmentation': (34.19, 8.2),\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "configs = list(ablation.keys())\n",
        "rmses = [v[0] for v in ablation.values()]\n",
        "changes = [v[1] for v in ablation.values()]\n",
        "\n",
        "colors = ['green' if c <= 0 else 'red' for c in changes]\n",
        "bars = ax.bar(configs, rmses, color=colors, alpha=0.7, edgecolor='black')\n",
        "\n",
        "# Add change annotations\n",
        "for bar, change in zip(bars, changes):\n",
        "    height = bar.get_height()\n",
        "    if change != 0:\n",
        "        sign = '+' if change > 0 else ''\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.3,\n",
        "                f'{sign}{change}%', ha='center', fontsize=9,\n",
        "                color='red' if change > 0 else 'green', fontweight='bold')\n",
        "\n",
        "ax.axhline(y=31.58, color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
        "ax.set_ylabel('Test RMSE (meters)')\n",
        "ax.set_title('Ablation Study: Effect of Different Augmentations')\n",
        "ax.set_ylim(28, 40)\n",
        "plt.xticks(rotation=20, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[TAKEAWAYS]\")\n",
        "print(\"   [+] Grayscale: Helps! Forces model to use geometry over color\")\n",
        "print(\"   [-] Blur: Hurts! Removes fine details needed for localization\")\n",
        "print(\"   [-] Erasing: Hurts! Occludes important spatial cues\")\n",
        "print(\"   [-] Combining aggressive augmentations makes things worse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzdNXSsL850Z"
      },
      "source": [
        "---\n",
        "## Part 7: Try It Yourself!\n",
        "\n",
        "Upload your own image and see where the model thinks it was taken.\n",
        "\n",
        "**Note**: Our model is trained only on Penn's campus (33rd & Walnut to 34th & Spruce), so it will always predict locations within that region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrye1KYJ850Z"
      },
      "outputs": [],
      "source": [
        "# Interactive prediction function\n",
        "def predict_location(image_path=None, show_map=True):\n",
        "    \"\"\"\n",
        "    Predict the GPS location of an image.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image file, or None to use sample\n",
        "        show_map: Whether to display location on map\n",
        "    \"\"\"\n",
        "    # Use sample image if none provided\n",
        "    if image_path is None:\n",
        "        print(\"Using sample image (no image provided)\")\n",
        "        img = sample_pil\n",
        "    else:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Preprocess\n",
        "    img_tensor = inference_transform(img).unsqueeze(0)\n",
        "\n",
        "    # Predict (note: model has random weights, so this is just a demo)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        coords = model.predict(img_tensor)\n",
        "        pred_lat = coords[0, 0].item()\n",
        "        pred_lon = coords[0, 1].item()\n",
        "\n",
        "    print(f\"\\n[PREDICTION] Predicted Location:\")\n",
        "    print(f\"   Latitude:  {pred_lat:.6f} N\")\n",
        "    print(f\"   Longitude: {pred_lon:.6f} W\")\n",
        "\n",
        "    # Show on map\n",
        "    if show_map:\n",
        "        m = folium.Map(location=[pred_lat, pred_lon], zoom_start=18)\n",
        "        folium.Marker(\n",
        "            [pred_lat, pred_lon],\n",
        "            popup=f'Predicted: ({pred_lat:.5f}, {pred_lon:.5f})',\n",
        "            icon=folium.Icon(color='red', icon='camera')\n",
        "        ).add_to(m)\n",
        "\n",
        "        # Add accuracy circle (typical error ~22m)\n",
        "        folium.Circle(\n",
        "            [pred_lat, pred_lon],\n",
        "            radius=22,  # meters\n",
        "            color='blue',\n",
        "            fill=True,\n",
        "            fill_opacity=0.2,\n",
        "            popup='~22m typical error'\n",
        "        ).add_to(m)\n",
        "\n",
        "        return m\n",
        "\n",
        "# Demo with our sample\n",
        "print(\"[DEMO] Predicting location from sample image\")\n",
        "print(\"   (Note: Model has random weights - just demonstrating the pipeline)\")\n",
        "predict_location()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkjVfqrc850Z"
      },
      "source": [
        "### Upload Your Own Image\n",
        "\n",
        "In Google Colab, you can upload an image using the file browser on the left, then run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZkgR3GK850Z"
      },
      "outputs": [],
      "source": [
        "# Uncomment and modify to use your own image:\n",
        "# predict_location('/content/your_image.jpg')\n",
        "\n",
        "# Or use Colab's file upload:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     predict_location(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP0VqQlL850Z"
      },
      "source": [
        "---\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Visual geolocation is hard** - small physical distances can look very similar in images\n",
        "\n",
        "2. **Transfer learning helps** - pretrained ImageNet weights give us a 30% performance boost\n",
        "\n",
        "3. **Not all augmentation is good** - grayscale helps, but blur and erasing hurt localization\n",
        "\n",
        "4. **CNNs learn meaningful features** - early layers detect edges, deep layers capture scene structure\n",
        "\n",
        "5. **Haversine distance is essential** - use proper spherical geometry for GPS coordinates!\n",
        "\n",
        "### Performance Achieved\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Final RMSE | **21.92 meters** |\n",
        "| Improvement over baseline | **75.1%** |\n",
        "| Best augmentation | Grayscale (10% probability) |\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Our Hugging Face Dataset](https://huggingface.co/datasets/rantyw/image2gps)\n",
        "- [ResNet Paper](https://arxiv.org/abs/1512.03385)\n",
        "- [Haversine Formula](https://en.wikipedia.org/wiki/Haversine_formula)\n",
        "\n",
        "---\n",
        "\n",
        "*Tutorial created for CIS 5190 Applied Machine Learning, Fall 2025*\n",
        "\n",
        "*Authors: Cecilia Chen, Ranty Wang, Xun Wang*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
